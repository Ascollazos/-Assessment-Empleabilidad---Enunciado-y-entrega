{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4da8b930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30fd502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('riwi_loader.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# CONFIGURACIÃ“N DE BASE DE DATOS - CAMBIA ESTOS VALORES\n",
    "DB_CONFIG = {\n",
    "    'database': 'riwi_analytics',\n",
    "    'user': 'postgres',\n",
    "    'password': 'Qwe.123*',  # âš ï¸ CAMBIA ESTO\n",
    "    'host': 'localhost',\n",
    "    'port': 5432\n",
    "}\n",
    "\n",
    "ARCHIVO_CSV_LIMPIO = \"riwi_datos_limpios.csv\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd115019",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostgreSQLLoaderFinal:\n",
    "    def __init__(self, db_config, csv_file):\n",
    "        self.db_config = db_config\n",
    "        self.csv_file = csv_file\n",
    "        self.estadisticas = {}\n",
    "        self.engine = None\n",
    "        self.conn = None\n",
    "        \n",
    "    def conectar_postgresql(self):\n",
    "        \"\"\"Establecer conexiÃ³n directa con psycopg2\"\"\"\n",
    "        try:\n",
    "            self.conn = psycopg2.connect(**self.db_config)\n",
    "            self.conn.autocommit = False\n",
    "            logger.info(\"âœ… ConexiÃ³n psycopg2 establecida\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error conexiÃ³n psycopg2: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def crear_engine_sqlalchemy(self):\n",
    "        \"\"\"Crear engine de SQLAlchemy para operaciones bulk\"\"\"\n",
    "        try:\n",
    "            self.engine = create_engine(\n",
    "                f\"postgresql://{self.db_config['user']}:{self.db_config['password']}@\"\n",
    "                f\"{self.db_config['host']}:{self.db_config['port']}/{self.db_config['database']}\",\n",
    "                pool_pre_ping=True,\n",
    "                echo=False\n",
    "            )\n",
    "            logger.info(\"âœ… Engine SQLAlchemy creado\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error creando engine: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def validar_csv(self):\n",
    "        \"\"\"Validar que el archivo CSV limpio existe y tiene datos\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(self.csv_file):\n",
    "                logger.error(f\"âŒ Archivo no encontrado: {self.csv_file}\")\n",
    "                return False\n",
    "            \n",
    "            # Verificar tamaÃ±o\n",
    "            tamano_mb = os.path.getsize(self.csv_file) / (1024 * 1024)\n",
    "            logger.info(f\"ğŸ“Š TamaÃ±o CSV: {tamano_mb:.2f} MB\")\n",
    "            \n",
    "            # Leer primeras filas para validar\n",
    "            df_sample = pd.read_csv(self.csv_file, nrows=100)\n",
    "            logger.info(f\"âœ… CSV vÃ¡lido encontrado: {self.csv_file}\")\n",
    "            logger.info(f\"ğŸ“‹ Columnas: {list(df_sample.columns)}\")\n",
    "            \n",
    "            # Contar total de filas\n",
    "            with open(self.csv_file, 'r', encoding='utf-8') as f:\n",
    "                total_filas = sum(1 for line in f) - 1  # Restar header\n",
    "            \n",
    "            logger.info(f\"ğŸ“ˆ Total filas: {total_filas:,}\")\n",
    "            self.estadisticas['total_filas_csv'] = total_filas\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error validando CSV: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def cargar_csv(self):\n",
    "        \"\"\"Cargar el CSV limpio con manejo de tipos de datos\"\"\"\n",
    "        try:\n",
    "            logger.info(\"ğŸ“– Cargando CSV limpio...\")\n",
    "            \n",
    "            # Leer CSV especificando tipos de datos para evitar errores\n",
    "            dtype_dict = {\n",
    "                'cantidad': 'float64',\n",
    "                'precio_unitario': 'float64',\n",
    "                'descuento': 'float64',\n",
    "                'costo_envio': 'float64',\n",
    "                'total': 'float64',\n",
    "                'anio': 'int64',\n",
    "                'mes': 'int64',\n",
    "                'trimestre': 'int64'\n",
    "            }\n",
    "            \n",
    "            df = pd.read_csv(self.csv_file, dtype=dtype_dict)\n",
    "            \n",
    "            # Asegurar que las columnas de texto sean strings y manejar NaN\n",
    "            columnas_texto = ['producto', 'tipo_producto', 'ciudad', 'pais', 'tipo_venta', 'tipo_cliente']\n",
    "            for col in columnas_texto:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].astype(str)\n",
    "                    # Reemplazar 'nan' strings con valores vacÃ­os\n",
    "                    df[col] = df[col].replace('nan', '')\n",
    "                    df[col] = df[col].str.strip()\n",
    "            \n",
    "            # Convertir fecha\n",
    "            if 'fecha' in df.columns:\n",
    "                df['fecha'] = pd.to_datetime(df['fecha'], errors='coerce')\n",
    "            \n",
    "            logger.info(f\"âœ… CSV cargado: {len(df)} registros, {len(df.columns)} columnas\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error cargando CSV: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def validar_columnas_tabla(self):\n",
    "        \"\"\"Verificar que las columnas de la tabla existen\"\"\"\n",
    "        try:\n",
    "            query = \"\"\"\n",
    "                SELECT column_name \n",
    "                FROM information_schema.columns \n",
    "                WHERE table_schema = 'analytics' \n",
    "                AND table_name = 'ventas'\n",
    "                ORDER BY ordinal_position\n",
    "            \"\"\"\n",
    "            columnas_existentes = pd.read_sql(query, self.engine)['column_name'].tolist()\n",
    "            logger.info(f\"ğŸ“‹ Columnas existentes en tabla ventas: {columnas_existentes}\")\n",
    "            \n",
    "            return columnas_existentes\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error verificando columnas: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def cargar_dimensiones_basicas(self):\n",
    "        \"\"\"Cargar datos bÃ¡sicos de dimensiones\"\"\"\n",
    "        try:\n",
    "            logger.info(\"ğŸ“Š Cargando dimensiones bÃ¡sicas...\")\n",
    "            \n",
    "            # Datos predefinidos para dimensiones\n",
    "            dimensiones_data = {\n",
    "                'tipos_producto': [\n",
    "                    (1, 'Hogar'), (2, 'Bebida'), (3, 'LÃ¡cteo'), \n",
    "                    (4, 'Alimento_Perecedero'), (5, 'Snacks'), (6, 'Abarrotes')\n",
    "                ],\n",
    "                'tipos_cliente': [\n",
    "                    (1, 'Gobierno'), (2, 'Corporativo'), (3, 'Minorista'), (4, 'Mayorista')\n",
    "                ],\n",
    "                'canales_venta': [\n",
    "                    (1, 'Tienda Fisica'), (2, 'Distribuidor'), (3, 'Call Center'), (4, 'Online')\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            with self.conn.cursor() as cursor:\n",
    "                # Cargar tipos de producto\n",
    "                for tipo_id, nombre in dimensiones_data['tipos_producto']:\n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT INTO analytics.tipos_producto (tipo_producto_id, nombre_tipo, categoria_principal)\n",
    "                        VALUES (%s, %s, %s)\n",
    "                        ON CONFLICT DO NOTHING\n",
    "                    \"\"\", (tipo_id, nombre, 'CategorÃ­a'))\n",
    "                \n",
    "                # Cargar tipos de cliente\n",
    "                for cliente_id, nombre in dimensiones_data['tipos_cliente']:\n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT INTO analytics.tipos_cliente (tipo_cliente_id, nombre_tipo, descripcion)\n",
    "                        VALUES (%s, %s, %s)\n",
    "                        ON CONFLICT DO NOTHING\n",
    "                    \"\"\", (cliente_id, nombre, f\"Cliente {nombre}\"))\n",
    "                \n",
    "                # Cargar canales de venta\n",
    "                for canal_id, nombre in dimensiones_data['canales_venta']:\n",
    "                    es_digital = nombre in ['Call Center', 'Online']\n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT INTO analytics.canales_venta (canal_id, nombre_canal, tipo_canal, es_digital)\n",
    "                        VALUES (%s, %s, %s, %s)\n",
    "                        ON CONFLICT DO NOTHING\n",
    "                    \"\"\", (canal_id, nombre, 'Principal', es_digital))\n",
    "            \n",
    "            self.conn.commit()\n",
    "            logger.info(\"âœ… Dimensiones bÃ¡sicas cargadas\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error cargando dimensiones bÃ¡sicas: {e}\")\n",
    "            self.conn.rollback()\n",
    "            return False\n",
    "    \n",
    "    def procesar_y_cargar(self, df):\n",
    "        \"\"\"Procesar dataframe y cargar a PostgreSQL con mapeo correcto de columnas\"\"\"\n",
    "        try:\n",
    "            logger.info(\"ğŸ”„ Procesando datos para carga...\")\n",
    "            \n",
    "            # 1. CARGAR REGIONES ÃšNICAS\n",
    "            logger.info(\"ğŸŒ Cargando regiones...\")\n",
    "            regiones_unicas = df[['pais', 'ciudad']].drop_duplicates().reset_index(drop=True)\n",
    "            regiones_unicas['region_id'] = range(1, len(regiones_unicas) + 1)\n",
    "            \n",
    "            # Limpiar datos de regiÃ³n\n",
    "            regiones_unicas['pais'] = regiones_unicas['pais'].astype(str).str.strip().str.upper()\n",
    "            regiones_unicas['ciudad'] = regiones_unicas['ciudad'].astype(str).str.strip().str.upper()\n",
    "            \n",
    "            # Cargar regiones\n",
    "            regiones_data = regiones_unicas[['region_id', 'pais', 'ciudad']].values.tolist()\n",
    "            with self.conn.cursor() as cursor:\n",
    "                for region_id, pais, ciudad in regiones_data:\n",
    "                    # Manejar valores None o vacÃ­os\n",
    "                    if not pais or pais == 'NAN':\n",
    "                        pais = 'SIN_PAIS'\n",
    "                    if not ciudad or ciudad == 'NAN':\n",
    "                        ciudad = 'SIN_CIUDAD'\n",
    "                    \n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT INTO analytics.regiones (region_id, pais, ciudad, activo, fecha_creacion)\n",
    "                        VALUES (%s, %s, %s, %s, %s)\n",
    "                        ON CONFLICT DO NOTHING\n",
    "                    \"\"\", (region_id, pais, ciudad, True, datetime.now()))\n",
    "            self.conn.commit()\n",
    "            logger.info(f\"âœ… {len(regiones_unicas)} regiones cargadas\")\n",
    "            \n",
    "            # 2. OBTENER MAPA DE REGIONES\n",
    "            regiones_map = pd.read_sql(\"SELECT region_id, pais, ciudad FROM analytics.regiones\", self.engine)\n",
    "            df = df.merge(regiones_map, on=['pais', 'ciudad'], how='left')\n",
    "            \n",
    "            # 3. CARGAR PRODUCTOS ÃšNICOS\n",
    "            logger.info(\"ğŸ“¦ Cargando productos...\")\n",
    "            productos_unicos = df[['producto', 'tipo_producto']].drop_duplicates().reset_index(drop=True)\n",
    "            productos_unicos['producto_id'] = range(1, len(productos_unicos) + 1)\n",
    "            \n",
    "            # Limpiar datos de producto\n",
    "            productos_unicos['producto'] = productos_unicos['producto'].astype(str).str.strip()\n",
    "            productos_unicos['tipo_producto'] = productos_unicos['tipo_producto'].astype(str).str.strip()\n",
    "            \n",
    "            # Cargar productos\n",
    "            productos_data = productos_unicos[['producto_id', 'producto', 'tipo_producto']].values.tolist()\n",
    "            with self.conn.cursor() as cursor:\n",
    "                for producto_id, nombre, tipo in productos_data:\n",
    "                    # Manejar valores None o vacÃ­os\n",
    "                    if not nombre or nombre == 'nan':\n",
    "                        nombre = 'SIN_NOMBRE'\n",
    "                    if not tipo or tipo == 'nan':\n",
    "                        tipo = 'SIN_TIPO'\n",
    "                    \n",
    "                    cursor.execute(\"\"\"\n",
    "                        INSERT INTO analytics.productos (producto_id, nombre_producto, categoria_sistema, activo, fecha_creacion)\n",
    "                        VALUES (%s, %s, %s, %s, %s)\n",
    "                        ON CONFLICT DO NOTHING\n",
    "                    \"\"\", (producto_id, nombre.title(), tipo.title(), True, datetime.now()))\n",
    "            self.conn.commit()\n",
    "            logger.info(f\"âœ… {len(productos_unicos)} productos cargados\")\n",
    "            \n",
    "            # 4. OBTENER MAPA DE PRODUCTOS\n",
    "            productos_map = pd.read_sql(\"SELECT producto_id, nombre_producto FROM analytics.productos\", self.engine)\n",
    "            df = df.merge(productos_map, left_on='producto', right_on='nombre_producto', how='left')\n",
    "            \n",
    "            # 5. PREPARAR VENTAS CON MAPEO CORRECTO DE COLUMNAS\n",
    "            logger.info(\"ğŸ’° Preparando ventas...\")\n",
    "            \n",
    "            # Mapear tipos de cliente y venta a IDs\n",
    "            cliente_map = {\n",
    "                'Gobierno': 1, 'Corporativo': 2, 'Minorista': 3, 'Mayorista': 4,\n",
    "                'Tienda Fisica': 1, 'Distribuidor': 2, 'Call Center': 3, 'Online': 4\n",
    "            }\n",
    "            \n",
    "            # Preparar dataframe de ventas con nombres de columnas exactos de la BD\n",
    "            ventas_df = pd.DataFrame()\n",
    "            ventas_df['fecha_venta'] = pd.to_datetime(df['fecha'], errors='coerce')\n",
    "            ventas_df['producto_id'] = pd.to_numeric(df['producto_id'], errors='coerce').fillna(0).astype(int)\n",
    "            ventas_df['region_id'] = pd.to_numeric(df['region_id'], errors='coerce').fillna(0).astype(int)\n",
    "            \n",
    "            # Mapear tipos con manejo de valores faltantes\n",
    "            ventas_df['canal_id'] = df['tipo_venta'].map(cliente_map).fillna(1).astype(int)\n",
    "            ventas_df['tipo_cliente_id'] = df['tipo_cliente'].map(cliente_map).fillna(1).astype(int)\n",
    "            \n",
    "            # Columnas numÃ©ricas con nombres exactos de la BD\n",
    "            ventas_df['cantidad'] = pd.to_numeric(df['cantidad'], errors='coerce').fillna(0)\n",
    "            ventas_df['precio_unitario'] = pd.to_numeric(df['precio_unitario'], errors='coerce').fillna(0)\n",
    "            ventas_df['descuento_porcentaje'] = pd.to_numeric(df['descuento'], errors='coerce').fillna(0)\n",
    "            ventas_df['costo_envio'] = pd.to_numeric(df['costo_envio'], errors='coerce').fillna(0)\n",
    "            ventas_df['monto_total'] = pd.to_numeric(df['total'], errors='coerce').fillna(0)\n",
    "            ventas_df['fecha_carga'] = datetime.now()\n",
    "            \n",
    "            # Eliminar filas con datos crÃ­ticos faltantes\n",
    "            ventas_df = ventas_df.dropna(subset=['fecha_venta', 'producto_id', 'region_id'])\n",
    "            ventas_df = ventas_df[(ventas_df['producto_id'] > 0) & (ventas_df['region_id'] > 0)]\n",
    "            \n",
    "            # 6. CARGAR VENTAS EN CHUNKS\n",
    "            logger.info(\"ğŸ“¤ Cargando ventas...\")\n",
    "            chunk_size = 5000\n",
    "            total_chunks = (len(ventas_df) // chunk_size) + 1\n",
    "            \n",
    "            for i in range(0, len(ventas_df), chunk_size):\n",
    "                chunk = ventas_df.iloc[i:i+chunk_size]\n",
    "                \n",
    "                # Cargar chunk con columnas exactas\n",
    "                chunk.to_sql(\n",
    "                    'ventas', self.engine, schema='analytics',\n",
    "                    if_exists='append', index=False, method='multi',\n",
    "                    chunksize=1000\n",
    "                )\n",
    "                \n",
    "                logger.info(f\"ğŸ“ˆ Chunk {i//chunk_size + 1}/{total_chunks}: {len(chunk)} registros\")\n",
    "            \n",
    "            self.estadisticas['total_ventas'] = len(ventas_df)\n",
    "            logger.info(f\"âœ… {len(ventas_df)} ventas cargadas\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error en proceso de carga: {e}\")\n",
    "            import traceback\n",
    "            logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "            self.conn.rollback()\n",
    "            return False\n",
    "    \n",
    "    def crear_indices_optimizacion(self):\n",
    "        \"\"\"Crear Ã­ndices para optimizar consultas\"\"\"\n",
    "        try:\n",
    "            logger.info(\"ğŸ”‘ Creando Ã­ndices de optimizaciÃ³n...\")\n",
    "            \n",
    "            queries_indice = [\n",
    "                \"CREATE INDEX IF NOT EXISTS idx_ventas_fecha ON analytics.ventas(fecha_venta)\",\n",
    "                \"CREATE INDEX IF NOT EXISTS idx_ventas_producto ON analytics.ventas(producto_id)\",\n",
    "                \"CREATE INDEX IF NOT EXISTS idx_ventas_region ON analytics.ventas(region_id)\",\n",
    "                \"CREATE INDEX IF NOT EXISTS idx_ventas_canal ON analytics.ventas(canal_id)\",\n",
    "                \"CREATE INDEX IF NOT EXISTS idx_ventas_cliente ON analytics.ventas(tipo_cliente_id)\",\n",
    "                \"CREATE INDEX IF NOT EXISTS idx_productos_nombre ON analytics.productos(nombre_producto)\",\n",
    "                \"CREATE INDEX IF NOT EXISTS idx_regiones_pais_ciudad ON analytics.regiones(pais, ciudad)\"\n",
    "            ]\n",
    "            \n",
    "            with self.conn.cursor() as cursor:\n",
    "                for query in queries_indice:\n",
    "                    cursor.execute(query)\n",
    "            \n",
    "            self.conn.commit()\n",
    "            logger.info(\"âœ… Ãndices creados exitosamente\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error creando Ã­ndices: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def verificar_carga(self):\n",
    "        \"\"\"Verificar que los datos se cargaron correctamente\"\"\"\n",
    "        try:\n",
    "            logger.info(\"ğŸ” Verificando carga de datos...\")\n",
    "            \n",
    "            queries_verificacion = {\n",
    "                'total_ventas': \"SELECT COUNT(*) as total FROM analytics.ventas\",\n",
    "                'total_productos': \"SELECT COUNT(*) as total FROM analytics.productos\",\n",
    "                'total_regiones': \"SELECT COUNT(*) as total FROM analytics.regiones\",\n",
    "                'rango_fechas': \"\"\"\n",
    "                    SELECT \n",
    "                        MIN(fecha_venta) as fecha_min,\n",
    "                        MAX(fecha_venta) as fecha_max\n",
    "                    FROM analytics.ventas\n",
    "                \"\"\",\n",
    "                'ventas_por_pais': \"\"\"\n",
    "                    SELECT pais, COUNT(*) as total_ventas\n",
    "                    FROM analytics.ventas v\n",
    "                    JOIN analytics.regiones r ON v.region_id = r.region_id\n",
    "                    GROUP BY pais\n",
    "                    ORDER BY total_ventas DESC\n",
    "                    LIMIT 5\n",
    "                \"\"\"\n",
    "            }\n",
    "            \n",
    "            resultados = {}\n",
    "            for nombre, query in queries_verificacion.items():\n",
    "                resultado = pd.read_sql(query, self.engine)\n",
    "                resultados[nombre] = resultado\n",
    "            \n",
    "            # Mostrar resultados\n",
    "            logger.info(\"ğŸ“Š VERIFICACIÃ“N DE CARGA:\")\n",
    "            logger.info(f\"  ğŸ“ˆ Total ventas: {resultados['total_ventas'].iloc[0]['total']:,}\")\n",
    "            logger.info(f\"  ğŸ“¦ Total productos: {resultados['total_productos'].iloc[0]['total']:,}\")\n",
    "            logger.info(f\"  ğŸŒ Total regiones: {resultados['total_regiones'].iloc[0]['total']:,}\")\n",
    "            \n",
    "            fecha_min = resultados['rango_fechas'].iloc[0]['fecha_min']\n",
    "            fecha_max = resultados['rango_fechas'].iloc[0]['fecha_max']\n",
    "            logger.info(f\"  ğŸ“… Rango fechas: {fecha_min} hasta {fecha_max}\")\n",
    "            \n",
    "            logger.info(\"  ğŸŒ Top 5 paÃ­ses:\")\n",
    "            for _, row in resultados['ventas_por_pais'].iterrows():\n",
    "                logger.info(f\"    {row['pais']}: {row['total_ventas']:,} ventas\")\n",
    "            \n",
    "            # Guardar estadÃ­sticas\n",
    "            self.estadisticas.update({\n",
    "                'ventas_finales': int(resultados['total_ventas'].iloc[0]['total']),\n",
    "                'productos_finales': int(resultados['total_productos'].iloc[0]['total']),\n",
    "                'regiones_finales': int(resultados['total_regiones'].iloc[0]['total']),\n",
    "                'fecha_min': str(fecha_min),\n",
    "                'fecha_max': str(fecha_max)\n",
    "            })\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error en verificaciÃ³n: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def guardar_estadisticas(self):\n",
    "        \"\"\"Guardar estadÃ­sticas del proceso\"\"\"\n",
    "        try:\n",
    "            estadisticas_finales = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'archivo_csv': self.csv_file,\n",
    "                'estadisticas': self.estadisticas,\n",
    "                'base_datos': self.db_config['database']\n",
    "            }\n",
    "            \n",
    "            with open('riwi_carga_estadisticas.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(estadisticas_finales, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            logger.info(\"ğŸ“Š EstadÃ­sticas guardadas en riwi_carga_estadisticas.json\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error guardando estadÃ­sticas: {e}\")\n",
    "    \n",
    "    def ejecutar_carga_completa(self):\n",
    "        \"\"\"Ejecutar proceso completo de carga\"\"\"\n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"ğŸš€ INICIANDO CARGA FINAL A POSTGRESQL\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Validar CSV\n",
    "        if not self.validar_csv():\n",
    "            return False\n",
    "        \n",
    "        # Establecer conexiones\n",
    "        if not self.conectar_postgresql():\n",
    "            return False\n",
    "        \n",
    "        if not self.crear_engine_sqlalchemy():\n",
    "            return False\n",
    "        \n",
    "        # Cargar CSV\n",
    "        df = self.cargar_csv()\n",
    "        if df is None:\n",
    "            return False\n",
    "        \n",
    "        # Cargar dimensiones bÃ¡sicas\n",
    "        if not self.cargar_dimensiones_basicas():\n",
    "            return False\n",
    "        \n",
    "        # Procesar y cargar datos principales\n",
    "        if not self.procesar_y_cargar(df):\n",
    "            return False\n",
    "        \n",
    "        # Crear Ã­ndices\n",
    "        self.crear_indices_optimizacion()\n",
    "        \n",
    "        # Verificar carga\n",
    "        self.verificar_carga()\n",
    "        \n",
    "        # Guardar estadÃ­sticas\n",
    "        self.guardar_estadisticas()\n",
    "        \n",
    "        logger.info(\"=\" * 60)\n",
    "        logger.info(\"âœ… CARGA FINAL COMPLETADA EXITOSAMENTE\")\n",
    "        logger.info(\"=\" * 60)\n",
    "        \n",
    "        # Cerrar conexiones\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "        \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60da0134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-05 17:17:26,869 - INFO - ============================================================\n",
      "2026-01-05 17:17:26,870 - INFO - ğŸš€ INICIANDO CARGA FINAL A POSTGRESQL\n",
      "2026-01-05 17:17:26,870 - INFO - ============================================================\n",
      "2026-01-05 17:17:26,870 - INFO - ğŸ“Š TamaÃ±o CSV: 272.27 MB\n",
      "2026-01-05 17:17:26,875 - INFO - âœ… CSV vÃ¡lido encontrado: riwi_datos_limpios.csv\n",
      "2026-01-05 17:17:26,875 - INFO - ğŸ“‹ Columnas: ['fecha', 'producto', 'tipo_producto', 'cantidad', 'precio_unitario', 'ciudad', 'pais', 'tipo_venta', 'tipo_cliente', 'descuento', 'costo_envio', 'total', 'anio', 'mes', 'trimestre', 'nombre_mes', 'dia_semana', 'subtotal', 'monto_descuento', 'total_calculado']\n",
      "2026-01-05 17:17:27,173 - INFO - ğŸ“ˆ Total filas: 1,935,733\n",
      "2026-01-05 17:17:27,181 - INFO - âœ… ConexiÃ³n psycopg2 establecida\n",
      "2026-01-05 17:17:27,200 - INFO - âœ… Engine SQLAlchemy creado\n",
      "2026-01-05 17:17:27,201 - INFO - ğŸ“– Cargando CSV limpio...\n",
      "2026-01-05 17:17:31,078 - INFO - âœ… CSV cargado: 1935733 registros, 20 columnas\n",
      "2026-01-05 17:17:31,079 - INFO - ğŸ“Š Cargando dimensiones bÃ¡sicas...\n",
      "2026-01-05 17:17:31,085 - INFO - âœ… Dimensiones bÃ¡sicas cargadas\n",
      "2026-01-05 17:17:31,085 - INFO - ğŸ”„ Procesando datos para carga...\n",
      "2026-01-05 17:17:31,085 - INFO - ğŸŒ Cargando regiones...\n",
      "2026-01-05 17:17:31,276 - INFO - âœ… 21 regiones cargadas\n",
      "2026-01-05 17:17:31,698 - INFO - ğŸ“¦ Cargando productos...\n",
      "2026-01-05 17:17:31,871 - INFO - âœ… 84 productos cargados\n",
      "2026-01-05 17:17:32,141 - INFO - ğŸ’° Preparando ventas...\n",
      "2026-01-05 17:17:32,487 - INFO - ğŸ“¤ Cargando ventas...\n",
      "2026-01-05 17:17:33,091 - INFO - ğŸ“ˆ Chunk 1/388: 5000 registros\n",
      "2026-01-05 17:17:33,694 - INFO - ğŸ“ˆ Chunk 2/388: 5000 registros\n",
      "2026-01-05 17:17:34,288 - INFO - ğŸ“ˆ Chunk 3/388: 5000 registros\n",
      "2026-01-05 17:17:34,866 - INFO - ğŸ“ˆ Chunk 4/388: 5000 registros\n",
      "2026-01-05 17:17:35,441 - INFO - ğŸ“ˆ Chunk 5/388: 5000 registros\n",
      "2026-01-05 17:17:36,039 - INFO - ğŸ“ˆ Chunk 6/388: 5000 registros\n",
      "2026-01-05 17:17:36,611 - INFO - ğŸ“ˆ Chunk 7/388: 5000 registros\n",
      "2026-01-05 17:17:37,227 - INFO - ğŸ“ˆ Chunk 8/388: 5000 registros\n",
      "2026-01-05 17:17:37,799 - INFO - ğŸ“ˆ Chunk 9/388: 5000 registros\n",
      "2026-01-05 17:17:38,393 - INFO - ğŸ“ˆ Chunk 10/388: 5000 registros\n",
      "2026-01-05 17:17:39,009 - INFO - ğŸ“ˆ Chunk 11/388: 5000 registros\n",
      "2026-01-05 17:17:39,616 - INFO - ğŸ“ˆ Chunk 12/388: 5000 registros\n",
      "2026-01-05 17:17:40,220 - INFO - ğŸ“ˆ Chunk 13/388: 5000 registros\n",
      "2026-01-05 17:17:40,808 - INFO - ğŸ“ˆ Chunk 14/388: 5000 registros\n",
      "2026-01-05 17:17:41,481 - INFO - ğŸ“ˆ Chunk 15/388: 5000 registros\n",
      "2026-01-05 17:17:42,070 - INFO - ğŸ“ˆ Chunk 16/388: 5000 registros\n",
      "2026-01-05 17:17:42,660 - INFO - ğŸ“ˆ Chunk 17/388: 5000 registros\n",
      "2026-01-05 17:17:43,236 - INFO - ğŸ“ˆ Chunk 18/388: 5000 registros\n",
      "2026-01-05 17:17:43,816 - INFO - ğŸ“ˆ Chunk 19/388: 5000 registros\n",
      "2026-01-05 17:17:44,432 - INFO - ğŸ“ˆ Chunk 20/388: 5000 registros\n",
      "2026-01-05 17:17:44,997 - INFO - ğŸ“ˆ Chunk 21/388: 5000 registros\n",
      "2026-01-05 17:17:45,562 - INFO - ğŸ“ˆ Chunk 22/388: 5000 registros\n",
      "2026-01-05 17:17:46,197 - INFO - ğŸ“ˆ Chunk 23/388: 5000 registros\n",
      "2026-01-05 17:17:46,813 - INFO - ğŸ“ˆ Chunk 24/388: 5000 registros\n",
      "2026-01-05 17:17:47,377 - INFO - ğŸ“ˆ Chunk 25/388: 5000 registros\n",
      "2026-01-05 17:17:47,944 - INFO - ğŸ“ˆ Chunk 26/388: 5000 registros\n",
      "2026-01-05 17:17:48,512 - INFO - ğŸ“ˆ Chunk 27/388: 5000 registros\n",
      "2026-01-05 17:17:49,103 - INFO - ğŸ“ˆ Chunk 28/388: 5000 registros\n",
      "2026-01-05 17:17:49,720 - INFO - ğŸ“ˆ Chunk 29/388: 5000 registros\n",
      "2026-01-05 17:17:50,360 - INFO - ğŸ“ˆ Chunk 30/388: 5000 registros\n",
      "2026-01-05 17:17:50,992 - INFO - ğŸ“ˆ Chunk 31/388: 5000 registros\n",
      "2026-01-05 17:17:51,584 - INFO - ğŸ“ˆ Chunk 32/388: 5000 registros\n",
      "2026-01-05 17:17:52,194 - INFO - ğŸ“ˆ Chunk 33/388: 5000 registros\n",
      "2026-01-05 17:17:52,807 - INFO - ğŸ“ˆ Chunk 34/388: 5000 registros\n",
      "2026-01-05 17:17:53,378 - INFO - ğŸ“ˆ Chunk 35/388: 5000 registros\n",
      "2026-01-05 17:17:53,945 - INFO - ğŸ“ˆ Chunk 36/388: 5000 registros\n",
      "2026-01-05 17:17:54,556 - INFO - ğŸ“ˆ Chunk 37/388: 5000 registros\n",
      "2026-01-05 17:17:55,185 - INFO - ğŸ“ˆ Chunk 38/388: 5000 registros\n",
      "2026-01-05 17:17:55,761 - INFO - ğŸ“ˆ Chunk 39/388: 5000 registros\n",
      "2026-01-05 17:17:56,350 - INFO - ğŸ“ˆ Chunk 40/388: 5000 registros\n",
      "2026-01-05 17:17:56,925 - INFO - ğŸ“ˆ Chunk 41/388: 5000 registros\n",
      "2026-01-05 17:17:57,526 - INFO - ğŸ“ˆ Chunk 42/388: 5000 registros\n",
      "2026-01-05 17:17:58,112 - INFO - ğŸ“ˆ Chunk 43/388: 5000 registros\n",
      "2026-01-05 17:17:58,720 - INFO - ğŸ“ˆ Chunk 44/388: 5000 registros\n",
      "2026-01-05 17:17:59,293 - INFO - ğŸ“ˆ Chunk 45/388: 5000 registros\n",
      "2026-01-05 17:17:59,863 - INFO - ğŸ“ˆ Chunk 46/388: 5000 registros\n",
      "2026-01-05 17:18:00,484 - INFO - ğŸ“ˆ Chunk 47/388: 5000 registros\n",
      "2026-01-05 17:18:01,049 - INFO - ğŸ“ˆ Chunk 48/388: 5000 registros\n",
      "2026-01-05 17:18:01,625 - INFO - ğŸ“ˆ Chunk 49/388: 5000 registros\n",
      "2026-01-05 17:18:02,199 - INFO - ğŸ“ˆ Chunk 50/388: 5000 registros\n",
      "2026-01-05 17:18:02,815 - INFO - ğŸ“ˆ Chunk 51/388: 5000 registros\n",
      "2026-01-05 17:18:03,412 - INFO - ğŸ“ˆ Chunk 52/388: 5000 registros\n",
      "2026-01-05 17:18:03,979 - INFO - ğŸ“ˆ Chunk 53/388: 5000 registros\n",
      "2026-01-05 17:18:04,564 - INFO - ğŸ“ˆ Chunk 54/388: 5000 registros\n",
      "2026-01-05 17:18:05,143 - INFO - ğŸ“ˆ Chunk 55/388: 5000 registros\n",
      "2026-01-05 17:18:05,747 - INFO - ğŸ“ˆ Chunk 56/388: 5000 registros\n",
      "2026-01-05 17:18:06,335 - INFO - ğŸ“ˆ Chunk 57/388: 5000 registros\n",
      "2026-01-05 17:18:06,913 - INFO - ğŸ“ˆ Chunk 58/388: 5000 registros\n",
      "2026-01-05 17:18:07,548 - INFO - ğŸ“ˆ Chunk 59/388: 5000 registros\n",
      "2026-01-05 17:18:08,109 - INFO - ğŸ“ˆ Chunk 60/388: 5000 registros\n",
      "2026-01-05 17:18:08,712 - INFO - ğŸ“ˆ Chunk 61/388: 5000 registros\n",
      "2026-01-05 17:18:09,287 - INFO - ğŸ“ˆ Chunk 62/388: 5000 registros\n",
      "2026-01-05 17:18:09,868 - INFO - ğŸ“ˆ Chunk 63/388: 5000 registros\n",
      "2026-01-05 17:18:10,457 - INFO - ğŸ“ˆ Chunk 64/388: 5000 registros\n",
      "2026-01-05 17:18:11,062 - INFO - ğŸ“ˆ Chunk 65/388: 5000 registros\n",
      "2026-01-05 17:18:11,685 - INFO - ğŸ“ˆ Chunk 66/388: 5000 registros\n",
      "2026-01-05 17:18:12,255 - INFO - ğŸ“ˆ Chunk 67/388: 5000 registros\n",
      "2026-01-05 17:18:12,846 - INFO - ğŸ“ˆ Chunk 68/388: 5000 registros\n",
      "2026-01-05 17:18:13,420 - INFO - ğŸ“ˆ Chunk 69/388: 5000 registros\n",
      "2026-01-05 17:18:14,020 - INFO - ğŸ“ˆ Chunk 70/388: 5000 registros\n",
      "2026-01-05 17:18:14,596 - INFO - ğŸ“ˆ Chunk 71/388: 5000 registros\n",
      "2026-01-05 17:18:15,169 - INFO - ğŸ“ˆ Chunk 72/388: 5000 registros\n",
      "2026-01-05 17:18:15,740 - INFO - ğŸ“ˆ Chunk 73/388: 5000 registros\n",
      "2026-01-05 17:18:16,448 - INFO - ğŸ“ˆ Chunk 74/388: 5000 registros\n",
      "2026-01-05 17:18:17,060 - INFO - ğŸ“ˆ Chunk 75/388: 5000 registros\n",
      "2026-01-05 17:18:17,687 - INFO - ğŸ“ˆ Chunk 76/388: 5000 registros\n",
      "2026-01-05 17:18:18,295 - INFO - ğŸ“ˆ Chunk 77/388: 5000 registros\n",
      "2026-01-05 17:18:18,896 - INFO - ğŸ“ˆ Chunk 78/388: 5000 registros\n",
      "2026-01-05 17:18:19,541 - INFO - ğŸ“ˆ Chunk 79/388: 5000 registros\n",
      "2026-01-05 17:18:20,182 - INFO - ğŸ“ˆ Chunk 80/388: 5000 registros\n",
      "2026-01-05 17:18:20,767 - INFO - ğŸ“ˆ Chunk 81/388: 5000 registros\n",
      "2026-01-05 17:18:21,353 - INFO - ğŸ“ˆ Chunk 82/388: 5000 registros\n",
      "2026-01-05 17:18:21,968 - INFO - ğŸ“ˆ Chunk 83/388: 5000 registros\n",
      "2026-01-05 17:18:22,550 - INFO - ğŸ“ˆ Chunk 84/388: 5000 registros\n",
      "2026-01-05 17:18:23,129 - INFO - ğŸ“ˆ Chunk 85/388: 5000 registros\n",
      "2026-01-05 17:18:23,707 - INFO - ğŸ“ˆ Chunk 86/388: 5000 registros\n",
      "2026-01-05 17:18:24,308 - INFO - ğŸ“ˆ Chunk 87/388: 5000 registros\n",
      "2026-01-05 17:18:24,943 - INFO - ğŸ“ˆ Chunk 88/388: 5000 registros\n",
      "2026-01-05 17:18:25,521 - INFO - ğŸ“ˆ Chunk 89/388: 5000 registros\n",
      "2026-01-05 17:18:26,105 - INFO - ğŸ“ˆ Chunk 90/388: 5000 registros\n",
      "2026-01-05 17:18:26,696 - INFO - ğŸ“ˆ Chunk 91/388: 5000 registros\n",
      "2026-01-05 17:18:27,271 - INFO - ğŸ“ˆ Chunk 92/388: 5000 registros\n",
      "2026-01-05 17:18:27,872 - INFO - ğŸ“ˆ Chunk 93/388: 5000 registros\n",
      "2026-01-05 17:18:28,456 - INFO - ğŸ“ˆ Chunk 94/388: 5000 registros\n",
      "2026-01-05 17:18:29,064 - INFO - ğŸ“ˆ Chunk 95/388: 5000 registros\n",
      "2026-01-05 17:18:29,633 - INFO - ğŸ“ˆ Chunk 96/388: 5000 registros\n",
      "2026-01-05 17:18:30,239 - INFO - ğŸ“ˆ Chunk 97/388: 5000 registros\n",
      "2026-01-05 17:18:30,811 - INFO - ğŸ“ˆ Chunk 98/388: 5000 registros\n",
      "2026-01-05 17:18:31,379 - INFO - ğŸ“ˆ Chunk 99/388: 5000 registros\n",
      "2026-01-05 17:18:31,965 - INFO - ğŸ“ˆ Chunk 100/388: 5000 registros\n",
      "2026-01-05 17:18:32,568 - INFO - ğŸ“ˆ Chunk 101/388: 5000 registros\n",
      "2026-01-05 17:18:33,174 - INFO - ğŸ“ˆ Chunk 102/388: 5000 registros\n",
      "2026-01-05 17:18:33,760 - INFO - ğŸ“ˆ Chunk 103/388: 5000 registros\n",
      "2026-01-05 17:18:34,348 - INFO - ğŸ“ˆ Chunk 104/388: 5000 registros\n",
      "2026-01-05 17:18:34,931 - INFO - ğŸ“ˆ Chunk 105/388: 5000 registros\n",
      "2026-01-05 17:18:35,561 - INFO - ğŸ“ˆ Chunk 106/388: 5000 registros\n",
      "2026-01-05 17:18:36,146 - INFO - ğŸ“ˆ Chunk 107/388: 5000 registros\n",
      "2026-01-05 17:18:36,744 - INFO - ğŸ“ˆ Chunk 108/388: 5000 registros\n",
      "2026-01-05 17:18:37,348 - INFO - ğŸ“ˆ Chunk 109/388: 5000 registros\n",
      "2026-01-05 17:18:37,908 - INFO - ğŸ“ˆ Chunk 110/388: 5000 registros\n",
      "2026-01-05 17:18:38,505 - INFO - ğŸ“ˆ Chunk 111/388: 5000 registros\n",
      "2026-01-05 17:18:39,070 - INFO - ğŸ“ˆ Chunk 112/388: 5000 registros\n",
      "2026-01-05 17:18:39,639 - INFO - ğŸ“ˆ Chunk 113/388: 5000 registros\n",
      "2026-01-05 17:18:40,226 - INFO - ğŸ“ˆ Chunk 114/388: 5000 registros\n",
      "2026-01-05 17:18:40,850 - INFO - ğŸ“ˆ Chunk 115/388: 5000 registros\n",
      "2026-01-05 17:18:41,450 - INFO - ğŸ“ˆ Chunk 116/388: 5000 registros\n",
      "2026-01-05 17:18:42,050 - INFO - ğŸ“ˆ Chunk 117/388: 5000 registros\n",
      "2026-01-05 17:18:42,659 - INFO - ğŸ“ˆ Chunk 118/388: 5000 registros\n",
      "2026-01-05 17:18:43,270 - INFO - ğŸ“ˆ Chunk 119/388: 5000 registros\n",
      "2026-01-05 17:18:43,890 - INFO - ğŸ“ˆ Chunk 120/388: 5000 registros\n",
      "2026-01-05 17:18:44,492 - INFO - ğŸ“ˆ Chunk 121/388: 5000 registros\n",
      "2026-01-05 17:18:45,120 - INFO - ğŸ“ˆ Chunk 122/388: 5000 registros\n",
      "2026-01-05 17:18:45,732 - INFO - ğŸ“ˆ Chunk 123/388: 5000 registros\n",
      "2026-01-05 17:18:46,369 - INFO - ğŸ“ˆ Chunk 124/388: 5000 registros\n",
      "2026-01-05 17:18:46,967 - INFO - ğŸ“ˆ Chunk 125/388: 5000 registros\n",
      "2026-01-05 17:18:47,552 - INFO - ğŸ“ˆ Chunk 126/388: 5000 registros\n",
      "2026-01-05 17:18:48,140 - INFO - ğŸ“ˆ Chunk 127/388: 5000 registros\n",
      "2026-01-05 17:18:48,744 - INFO - ğŸ“ˆ Chunk 128/388: 5000 registros\n",
      "2026-01-05 17:18:49,401 - INFO - ğŸ“ˆ Chunk 129/388: 5000 registros\n",
      "2026-01-05 17:18:50,033 - INFO - ğŸ“ˆ Chunk 130/388: 5000 registros\n",
      "2026-01-05 17:18:50,626 - INFO - ğŸ“ˆ Chunk 131/388: 5000 registros\n",
      "2026-01-05 17:18:51,220 - INFO - ğŸ“ˆ Chunk 132/388: 5000 registros\n",
      "2026-01-05 17:18:51,834 - INFO - ğŸ“ˆ Chunk 133/388: 5000 registros\n",
      "2026-01-05 17:18:52,408 - INFO - ğŸ“ˆ Chunk 134/388: 5000 registros\n",
      "2026-01-05 17:18:52,982 - INFO - ğŸ“ˆ Chunk 135/388: 5000 registros\n",
      "2026-01-05 17:18:53,553 - INFO - ğŸ“ˆ Chunk 136/388: 5000 registros\n",
      "2026-01-05 17:18:54,163 - INFO - ğŸ“ˆ Chunk 137/388: 5000 registros\n",
      "2026-01-05 17:18:54,779 - INFO - ğŸ“ˆ Chunk 138/388: 5000 registros\n",
      "2026-01-05 17:18:55,355 - INFO - ğŸ“ˆ Chunk 139/388: 5000 registros\n",
      "2026-01-05 17:18:55,941 - INFO - ğŸ“ˆ Chunk 140/388: 5000 registros\n",
      "2026-01-05 17:18:56,573 - INFO - ğŸ“ˆ Chunk 141/388: 5000 registros\n",
      "2026-01-05 17:18:57,232 - INFO - ğŸ“ˆ Chunk 142/388: 5000 registros\n",
      "2026-01-05 17:18:57,805 - INFO - ğŸ“ˆ Chunk 143/388: 5000 registros\n",
      "2026-01-05 17:18:58,432 - INFO - ğŸ“ˆ Chunk 144/388: 5000 registros\n",
      "2026-01-05 17:18:59,015 - INFO - ğŸ“ˆ Chunk 145/388: 5000 registros\n",
      "2026-01-05 17:18:59,595 - INFO - ğŸ“ˆ Chunk 146/388: 5000 registros\n",
      "2026-01-05 17:19:00,204 - INFO - ğŸ“ˆ Chunk 147/388: 5000 registros\n",
      "2026-01-05 17:19:00,795 - INFO - ğŸ“ˆ Chunk 148/388: 5000 registros\n",
      "2026-01-05 17:19:01,370 - INFO - ğŸ“ˆ Chunk 149/388: 5000 registros\n",
      "2026-01-05 17:19:01,952 - INFO - ğŸ“ˆ Chunk 150/388: 5000 registros\n",
      "2026-01-05 17:19:02,568 - INFO - ğŸ“ˆ Chunk 151/388: 5000 registros\n",
      "2026-01-05 17:19:03,206 - INFO - ğŸ“ˆ Chunk 152/388: 5000 registros\n",
      "2026-01-05 17:19:03,813 - INFO - ğŸ“ˆ Chunk 153/388: 5000 registros\n",
      "2026-01-05 17:19:04,412 - INFO - ğŸ“ˆ Chunk 154/388: 5000 registros\n",
      "2026-01-05 17:19:04,990 - INFO - ğŸ“ˆ Chunk 155/388: 5000 registros\n",
      "2026-01-05 17:19:05,593 - INFO - ğŸ“ˆ Chunk 156/388: 5000 registros\n",
      "2026-01-05 17:19:06,170 - INFO - ğŸ“ˆ Chunk 157/388: 5000 registros\n",
      "2026-01-05 17:19:06,749 - INFO - ğŸ“ˆ Chunk 158/388: 5000 registros\n",
      "2026-01-05 17:19:07,367 - INFO - ğŸ“ˆ Chunk 159/388: 5000 registros\n",
      "2026-01-05 17:19:07,964 - INFO - ğŸ“ˆ Chunk 160/388: 5000 registros\n",
      "2026-01-05 17:19:08,590 - INFO - ğŸ“ˆ Chunk 161/388: 5000 registros\n",
      "2026-01-05 17:19:09,165 - INFO - ğŸ“ˆ Chunk 162/388: 5000 registros\n",
      "2026-01-05 17:19:09,778 - INFO - ğŸ“ˆ Chunk 163/388: 5000 registros\n",
      "2026-01-05 17:19:10,369 - INFO - ğŸ“ˆ Chunk 164/388: 5000 registros\n",
      "2026-01-05 17:19:11,013 - INFO - ğŸ“ˆ Chunk 165/388: 5000 registros\n",
      "2026-01-05 17:19:11,614 - INFO - ğŸ“ˆ Chunk 166/388: 5000 registros\n",
      "2026-01-05 17:19:12,189 - INFO - ğŸ“ˆ Chunk 167/388: 5000 registros\n",
      "2026-01-05 17:19:12,766 - INFO - ğŸ“ˆ Chunk 168/388: 5000 registros\n",
      "2026-01-05 17:19:13,382 - INFO - ğŸ“ˆ Chunk 169/388: 5000 registros\n",
      "2026-01-05 17:19:13,961 - INFO - ğŸ“ˆ Chunk 170/388: 5000 registros\n",
      "2026-01-05 17:19:14,553 - INFO - ğŸ“ˆ Chunk 171/388: 5000 registros\n",
      "2026-01-05 17:19:15,128 - INFO - ğŸ“ˆ Chunk 172/388: 5000 registros\n",
      "2026-01-05 17:19:15,742 - INFO - ğŸ“ˆ Chunk 173/388: 5000 registros\n",
      "2026-01-05 17:19:16,370 - INFO - ğŸ“ˆ Chunk 174/388: 5000 registros\n",
      "2026-01-05 17:19:16,989 - INFO - ğŸ“ˆ Chunk 175/388: 5000 registros\n",
      "2026-01-05 17:19:17,599 - INFO - ğŸ“ˆ Chunk 176/388: 5000 registros\n",
      "2026-01-05 17:19:18,176 - INFO - ğŸ“ˆ Chunk 177/388: 5000 registros\n",
      "2026-01-05 17:19:18,823 - INFO - ğŸ“ˆ Chunk 178/388: 5000 registros\n",
      "2026-01-05 17:19:19,397 - INFO - ğŸ“ˆ Chunk 179/388: 5000 registros\n",
      "2026-01-05 17:19:19,999 - INFO - ğŸ“ˆ Chunk 180/388: 5000 registros\n",
      "2026-01-05 17:19:20,583 - INFO - ğŸ“ˆ Chunk 181/388: 5000 registros\n",
      "2026-01-05 17:19:21,166 - INFO - ğŸ“ˆ Chunk 182/388: 5000 registros\n",
      "2026-01-05 17:19:21,768 - INFO - ğŸ“ˆ Chunk 183/388: 5000 registros\n",
      "2026-01-05 17:19:22,376 - INFO - ğŸ“ˆ Chunk 184/388: 5000 registros\n",
      "2026-01-05 17:19:22,969 - INFO - ğŸ“ˆ Chunk 185/388: 5000 registros\n",
      "2026-01-05 17:19:23,556 - INFO - ğŸ“ˆ Chunk 186/388: 5000 registros\n",
      "2026-01-05 17:19:24,202 - INFO - ğŸ“ˆ Chunk 187/388: 5000 registros\n",
      "2026-01-05 17:19:24,787 - INFO - ğŸ“ˆ Chunk 188/388: 5000 registros\n",
      "2026-01-05 17:19:25,361 - INFO - ğŸ“ˆ Chunk 189/388: 5000 registros\n",
      "2026-01-05 17:19:25,931 - INFO - ğŸ“ˆ Chunk 190/388: 5000 registros\n",
      "2026-01-05 17:19:26,512 - INFO - ğŸ“ˆ Chunk 191/388: 5000 registros\n",
      "2026-01-05 17:19:27,135 - INFO - ğŸ“ˆ Chunk 192/388: 5000 registros\n",
      "2026-01-05 17:19:27,711 - INFO - ğŸ“ˆ Chunk 193/388: 5000 registros\n",
      "2026-01-05 17:19:28,300 - INFO - ğŸ“ˆ Chunk 194/388: 5000 registros\n",
      "2026-01-05 17:19:28,948 - INFO - ğŸ“ˆ Chunk 195/388: 5000 registros\n",
      "2026-01-05 17:19:29,585 - INFO - ğŸ“ˆ Chunk 196/388: 5000 registros\n",
      "2026-01-05 17:19:30,163 - INFO - ğŸ“ˆ Chunk 197/388: 5000 registros\n",
      "2026-01-05 17:19:30,756 - INFO - ğŸ“ˆ Chunk 198/388: 5000 registros\n",
      "2026-01-05 17:19:31,365 - INFO - ğŸ“ˆ Chunk 199/388: 5000 registros\n",
      "2026-01-05 17:19:31,942 - INFO - ğŸ“ˆ Chunk 200/388: 5000 registros\n",
      "2026-01-05 17:19:32,600 - INFO - ğŸ“ˆ Chunk 201/388: 5000 registros\n",
      "2026-01-05 17:19:33,194 - INFO - ğŸ“ˆ Chunk 202/388: 5000 registros\n",
      "2026-01-05 17:19:33,765 - INFO - ğŸ“ˆ Chunk 203/388: 5000 registros\n",
      "2026-01-05 17:19:34,357 - INFO - ğŸ“ˆ Chunk 204/388: 5000 registros\n",
      "2026-01-05 17:19:34,982 - INFO - ğŸ“ˆ Chunk 205/388: 5000 registros\n",
      "2026-01-05 17:19:35,559 - INFO - ğŸ“ˆ Chunk 206/388: 5000 registros\n",
      "2026-01-05 17:19:36,143 - INFO - ğŸ“ˆ Chunk 207/388: 5000 registros\n",
      "2026-01-05 17:19:36,726 - INFO - ğŸ“ˆ Chunk 208/388: 5000 registros\n",
      "2026-01-05 17:19:37,330 - INFO - ğŸ“ˆ Chunk 209/388: 5000 registros\n",
      "2026-01-05 17:19:37,930 - INFO - ğŸ“ˆ Chunk 210/388: 5000 registros\n",
      "2026-01-05 17:19:38,514 - INFO - ğŸ“ˆ Chunk 211/388: 5000 registros\n",
      "2026-01-05 17:19:39,098 - INFO - ğŸ“ˆ Chunk 212/388: 5000 registros\n",
      "2026-01-05 17:19:39,706 - INFO - ğŸ“ˆ Chunk 213/388: 5000 registros\n",
      "2026-01-05 17:19:40,329 - INFO - ğŸ“ˆ Chunk 214/388: 5000 registros\n",
      "2026-01-05 17:19:40,910 - INFO - ğŸ“ˆ Chunk 215/388: 5000 registros\n",
      "2026-01-05 17:19:41,517 - INFO - ğŸ“ˆ Chunk 216/388: 5000 registros\n",
      "2026-01-05 17:19:42,126 - INFO - ğŸ“ˆ Chunk 217/388: 5000 registros\n",
      "2026-01-05 17:19:42,716 - INFO - ğŸ“ˆ Chunk 218/388: 5000 registros\n",
      "2026-01-05 17:19:43,340 - INFO - ğŸ“ˆ Chunk 219/388: 5000 registros\n",
      "2026-01-05 17:19:43,963 - INFO - ğŸ“ˆ Chunk 220/388: 5000 registros\n",
      "2026-01-05 17:19:44,553 - INFO - ğŸ“ˆ Chunk 221/388: 5000 registros\n",
      "2026-01-05 17:19:45,153 - INFO - ğŸ“ˆ Chunk 222/388: 5000 registros\n",
      "2026-01-05 17:19:45,790 - INFO - ğŸ“ˆ Chunk 223/388: 5000 registros\n",
      "2026-01-05 17:19:46,368 - INFO - ğŸ“ˆ Chunk 224/388: 5000 registros\n",
      "2026-01-05 17:19:46,935 - INFO - ğŸ“ˆ Chunk 225/388: 5000 registros\n",
      "2026-01-05 17:19:47,499 - INFO - ğŸ“ˆ Chunk 226/388: 5000 registros\n",
      "2026-01-05 17:19:48,069 - INFO - ğŸ“ˆ Chunk 227/388: 5000 registros\n",
      "2026-01-05 17:19:48,694 - INFO - ğŸ“ˆ Chunk 228/388: 5000 registros\n",
      "2026-01-05 17:19:49,268 - INFO - ğŸ“ˆ Chunk 229/388: 5000 registros\n",
      "2026-01-05 17:19:49,862 - INFO - ğŸ“ˆ Chunk 230/388: 5000 registros\n",
      "2026-01-05 17:19:50,508 - INFO - ğŸ“ˆ Chunk 231/388: 5000 registros\n",
      "2026-01-05 17:19:51,166 - INFO - ğŸ“ˆ Chunk 232/388: 5000 registros\n",
      "2026-01-05 17:19:51,760 - INFO - ğŸ“ˆ Chunk 233/388: 5000 registros\n",
      "2026-01-05 17:19:52,336 - INFO - ğŸ“ˆ Chunk 234/388: 5000 registros\n",
      "2026-01-05 17:19:52,964 - INFO - ğŸ“ˆ Chunk 235/388: 5000 registros\n",
      "2026-01-05 17:19:53,557 - INFO - ğŸ“ˆ Chunk 236/388: 5000 registros\n",
      "2026-01-05 17:19:54,225 - INFO - ğŸ“ˆ Chunk 237/388: 5000 registros\n",
      "2026-01-05 17:19:54,799 - INFO - ğŸ“ˆ Chunk 238/388: 5000 registros\n",
      "2026-01-05 17:19:55,394 - INFO - ğŸ“ˆ Chunk 239/388: 5000 registros\n",
      "2026-01-05 17:19:55,974 - INFO - ğŸ“ˆ Chunk 240/388: 5000 registros\n",
      "2026-01-05 17:19:56,598 - INFO - ğŸ“ˆ Chunk 241/388: 5000 registros\n",
      "2026-01-05 17:19:57,178 - INFO - ğŸ“ˆ Chunk 242/388: 5000 registros\n",
      "2026-01-05 17:19:57,768 - INFO - ğŸ“ˆ Chunk 243/388: 5000 registros\n",
      "2026-01-05 17:19:58,381 - INFO - ğŸ“ˆ Chunk 244/388: 5000 registros\n",
      "2026-01-05 17:19:58,999 - INFO - ğŸ“ˆ Chunk 245/388: 5000 registros\n",
      "2026-01-05 17:19:59,596 - INFO - ğŸ“ˆ Chunk 246/388: 5000 registros\n",
      "2026-01-05 17:20:00,187 - INFO - ğŸ“ˆ Chunk 247/388: 5000 registros\n",
      "2026-01-05 17:20:00,764 - INFO - ğŸ“ˆ Chunk 248/388: 5000 registros\n",
      "2026-01-05 17:20:01,331 - INFO - ğŸ“ˆ Chunk 249/388: 5000 registros\n",
      "2026-01-05 17:20:01,934 - INFO - ğŸ“ˆ Chunk 250/388: 5000 registros\n",
      "2026-01-05 17:20:02,523 - INFO - ğŸ“ˆ Chunk 251/388: 5000 registros\n",
      "2026-01-05 17:20:03,125 - INFO - ğŸ“ˆ Chunk 252/388: 5000 registros\n",
      "2026-01-05 17:20:03,698 - INFO - ğŸ“ˆ Chunk 253/388: 5000 registros\n",
      "2026-01-05 17:20:04,269 - INFO - ğŸ“ˆ Chunk 254/388: 5000 registros\n",
      "2026-01-05 17:20:04,867 - INFO - ğŸ“ˆ Chunk 255/388: 5000 registros\n",
      "2026-01-05 17:20:05,475 - INFO - ğŸ“ˆ Chunk 256/388: 5000 registros\n",
      "2026-01-05 17:20:06,057 - INFO - ğŸ“ˆ Chunk 257/388: 5000 registros\n",
      "2026-01-05 17:20:06,643 - INFO - ğŸ“ˆ Chunk 258/388: 5000 registros\n",
      "2026-01-05 17:20:07,291 - INFO - ğŸ“ˆ Chunk 259/388: 5000 registros\n",
      "2026-01-05 17:20:07,865 - INFO - ğŸ“ˆ Chunk 260/388: 5000 registros\n",
      "2026-01-05 17:20:08,452 - INFO - ğŸ“ˆ Chunk 261/388: 5000 registros\n",
      "2026-01-05 17:20:09,054 - INFO - ğŸ“ˆ Chunk 262/388: 5000 registros\n",
      "2026-01-05 17:20:09,644 - INFO - ğŸ“ˆ Chunk 263/388: 5000 registros\n",
      "2026-01-05 17:20:10,256 - INFO - ğŸ“ˆ Chunk 264/388: 5000 registros\n",
      "2026-01-05 17:20:10,838 - INFO - ğŸ“ˆ Chunk 265/388: 5000 registros\n",
      "2026-01-05 17:20:11,449 - INFO - ğŸ“ˆ Chunk 266/388: 5000 registros\n",
      "2026-01-05 17:20:12,028 - INFO - ğŸ“ˆ Chunk 267/388: 5000 registros\n",
      "2026-01-05 17:20:12,656 - INFO - ğŸ“ˆ Chunk 268/388: 5000 registros\n",
      "2026-01-05 17:20:13,235 - INFO - ğŸ“ˆ Chunk 269/388: 5000 registros\n",
      "2026-01-05 17:20:13,820 - INFO - ğŸ“ˆ Chunk 270/388: 5000 registros\n",
      "2026-01-05 17:20:14,417 - INFO - ğŸ“ˆ Chunk 271/388: 5000 registros\n",
      "2026-01-05 17:20:14,990 - INFO - ğŸ“ˆ Chunk 272/388: 5000 registros\n",
      "2026-01-05 17:20:15,600 - INFO - ğŸ“ˆ Chunk 273/388: 5000 registros\n",
      "2026-01-05 17:20:16,211 - INFO - ğŸ“ˆ Chunk 274/388: 5000 registros\n",
      "2026-01-05 17:20:16,793 - INFO - ğŸ“ˆ Chunk 275/388: 5000 registros\n",
      "2026-01-05 17:20:17,380 - INFO - ğŸ“ˆ Chunk 276/388: 5000 registros\n",
      "2026-01-05 17:20:18,004 - INFO - ğŸ“ˆ Chunk 277/388: 5000 registros\n",
      "2026-01-05 17:20:18,589 - INFO - ğŸ“ˆ Chunk 278/388: 5000 registros\n",
      "2026-01-05 17:20:19,191 - INFO - ğŸ“ˆ Chunk 279/388: 5000 registros\n",
      "2026-01-05 17:20:19,778 - INFO - ğŸ“ˆ Chunk 280/388: 5000 registros\n",
      "2026-01-05 17:20:20,400 - INFO - ğŸ“ˆ Chunk 281/388: 5000 registros\n",
      "2026-01-05 17:20:21,021 - INFO - ğŸ“ˆ Chunk 282/388: 5000 registros\n",
      "2026-01-05 17:20:21,598 - INFO - ğŸ“ˆ Chunk 283/388: 5000 registros\n",
      "2026-01-05 17:20:22,188 - INFO - ğŸ“ˆ Chunk 284/388: 5000 registros\n",
      "2026-01-05 17:20:22,777 - INFO - ğŸ“ˆ Chunk 285/388: 5000 registros\n",
      "2026-01-05 17:20:23,381 - INFO - ğŸ“ˆ Chunk 286/388: 5000 registros\n",
      "2026-01-05 17:20:23,993 - INFO - ğŸ“ˆ Chunk 287/388: 5000 registros\n",
      "2026-01-05 17:20:24,613 - INFO - ğŸ“ˆ Chunk 288/388: 5000 registros\n",
      "2026-01-05 17:20:25,225 - INFO - ğŸ“ˆ Chunk 289/388: 5000 registros\n",
      "2026-01-05 17:20:25,806 - INFO - ğŸ“ˆ Chunk 290/388: 5000 registros\n",
      "2026-01-05 17:20:26,406 - INFO - ğŸ“ˆ Chunk 291/388: 5000 registros\n",
      "2026-01-05 17:20:27,013 - INFO - ğŸ“ˆ Chunk 292/388: 5000 registros\n",
      "2026-01-05 17:20:27,624 - INFO - ğŸ“ˆ Chunk 293/388: 5000 registros\n",
      "2026-01-05 17:20:28,207 - INFO - ğŸ“ˆ Chunk 294/388: 5000 registros\n",
      "2026-01-05 17:20:28,846 - INFO - ğŸ“ˆ Chunk 295/388: 5000 registros\n",
      "2026-01-05 17:20:29,443 - INFO - ğŸ“ˆ Chunk 296/388: 5000 registros\n",
      "2026-01-05 17:20:30,033 - INFO - ğŸ“ˆ Chunk 297/388: 5000 registros\n",
      "2026-01-05 17:20:30,639 - INFO - ğŸ“ˆ Chunk 298/388: 5000 registros\n",
      "2026-01-05 17:20:31,212 - INFO - ğŸ“ˆ Chunk 299/388: 5000 registros\n",
      "2026-01-05 17:20:31,813 - INFO - ğŸ“ˆ Chunk 300/388: 5000 registros\n",
      "2026-01-05 17:20:32,385 - INFO - ğŸ“ˆ Chunk 301/388: 5000 registros\n",
      "2026-01-05 17:20:33,010 - INFO - ğŸ“ˆ Chunk 302/388: 5000 registros\n",
      "2026-01-05 17:20:33,586 - INFO - ğŸ“ˆ Chunk 303/388: 5000 registros\n",
      "2026-01-05 17:20:34,198 - INFO - ğŸ“ˆ Chunk 304/388: 5000 registros\n",
      "2026-01-05 17:20:34,778 - INFO - ğŸ“ˆ Chunk 305/388: 5000 registros\n",
      "2026-01-05 17:20:35,352 - INFO - ğŸ“ˆ Chunk 306/388: 5000 registros\n",
      "2026-01-05 17:20:35,996 - INFO - ğŸ“ˆ Chunk 307/388: 5000 registros\n",
      "2026-01-05 17:20:36,574 - INFO - ğŸ“ˆ Chunk 308/388: 5000 registros\n",
      "2026-01-05 17:20:37,206 - INFO - ğŸ“ˆ Chunk 309/388: 5000 registros\n",
      "2026-01-05 17:20:37,785 - INFO - ğŸ“ˆ Chunk 310/388: 5000 registros\n",
      "2026-01-05 17:20:38,408 - INFO - ğŸ“ˆ Chunk 311/388: 5000 registros\n",
      "2026-01-05 17:20:38,988 - INFO - ğŸ“ˆ Chunk 312/388: 5000 registros\n",
      "2026-01-05 17:20:39,597 - INFO - ğŸ“ˆ Chunk 313/388: 5000 registros\n",
      "2026-01-05 17:20:40,187 - INFO - ğŸ“ˆ Chunk 314/388: 5000 registros\n",
      "2026-01-05 17:20:40,772 - INFO - ğŸ“ˆ Chunk 315/388: 5000 registros\n",
      "2026-01-05 17:20:41,352 - INFO - ğŸ“ˆ Chunk 316/388: 5000 registros\n",
      "2026-01-05 17:20:41,985 - INFO - ğŸ“ˆ Chunk 317/388: 5000 registros\n",
      "2026-01-05 17:20:42,604 - INFO - ğŸ“ˆ Chunk 318/388: 5000 registros\n",
      "2026-01-05 17:20:43,202 - INFO - ğŸ“ˆ Chunk 319/388: 5000 registros\n",
      "2026-01-05 17:20:43,790 - INFO - ğŸ“ˆ Chunk 320/388: 5000 registros\n",
      "2026-01-05 17:20:44,379 - INFO - ğŸ“ˆ Chunk 321/388: 5000 registros\n",
      "2026-01-05 17:20:44,964 - INFO - ğŸ“ˆ Chunk 322/388: 5000 registros\n",
      "2026-01-05 17:20:45,571 - INFO - ğŸ“ˆ Chunk 323/388: 5000 registros\n",
      "2026-01-05 17:20:46,182 - INFO - ğŸ“ˆ Chunk 324/388: 5000 registros\n",
      "2026-01-05 17:20:46,786 - INFO - ğŸ“ˆ Chunk 325/388: 5000 registros\n",
      "2026-01-05 17:20:47,366 - INFO - ğŸ“ˆ Chunk 326/388: 5000 registros\n",
      "2026-01-05 17:20:48,007 - INFO - ğŸ“ˆ Chunk 327/388: 5000 registros\n",
      "2026-01-05 17:20:48,580 - INFO - ğŸ“ˆ Chunk 328/388: 5000 registros\n",
      "2026-01-05 17:20:49,158 - INFO - ğŸ“ˆ Chunk 329/388: 5000 registros\n",
      "2026-01-05 17:20:49,736 - INFO - ğŸ“ˆ Chunk 330/388: 5000 registros\n",
      "2026-01-05 17:20:50,340 - INFO - ğŸ“ˆ Chunk 331/388: 5000 registros\n",
      "2026-01-05 17:20:50,993 - INFO - ğŸ“ˆ Chunk 332/388: 5000 registros\n",
      "2026-01-05 17:20:51,590 - INFO - ğŸ“ˆ Chunk 333/388: 5000 registros\n",
      "2026-01-05 17:20:52,174 - INFO - ğŸ“ˆ Chunk 334/388: 5000 registros\n",
      "2026-01-05 17:20:52,761 - INFO - ğŸ“ˆ Chunk 335/388: 5000 registros\n",
      "2026-01-05 17:20:53,397 - INFO - ğŸ“ˆ Chunk 336/388: 5000 registros\n",
      "2026-01-05 17:20:53,989 - INFO - ğŸ“ˆ Chunk 337/388: 5000 registros\n",
      "2026-01-05 17:20:54,611 - INFO - ğŸ“ˆ Chunk 338/388: 5000 registros\n",
      "2026-01-05 17:20:55,228 - INFO - ğŸ“ˆ Chunk 339/388: 5000 registros\n",
      "2026-01-05 17:20:55,811 - INFO - ğŸ“ˆ Chunk 340/388: 5000 registros\n",
      "2026-01-05 17:20:56,433 - INFO - ğŸ“ˆ Chunk 341/388: 5000 registros\n",
      "2026-01-05 17:20:57,005 - INFO - ğŸ“ˆ Chunk 342/388: 5000 registros\n",
      "2026-01-05 17:20:57,583 - INFO - ğŸ“ˆ Chunk 343/388: 5000 registros\n",
      "2026-01-05 17:20:58,167 - INFO - ğŸ“ˆ Chunk 344/388: 5000 registros\n",
      "2026-01-05 17:20:58,783 - INFO - ğŸ“ˆ Chunk 345/388: 5000 registros\n",
      "2026-01-05 17:20:59,390 - INFO - ğŸ“ˆ Chunk 346/388: 5000 registros\n",
      "2026-01-05 17:20:59,968 - INFO - ğŸ“ˆ Chunk 347/388: 5000 registros\n",
      "2026-01-05 17:21:00,555 - INFO - ğŸ“ˆ Chunk 348/388: 5000 registros\n",
      "2026-01-05 17:21:01,141 - INFO - ğŸ“ˆ Chunk 349/388: 5000 registros\n",
      "2026-01-05 17:21:01,757 - INFO - ğŸ“ˆ Chunk 350/388: 5000 registros\n",
      "2026-01-05 17:21:02,343 - INFO - ğŸ“ˆ Chunk 351/388: 5000 registros\n",
      "2026-01-05 17:21:02,984 - INFO - ğŸ“ˆ Chunk 352/388: 5000 registros\n",
      "2026-01-05 17:21:03,616 - INFO - ğŸ“ˆ Chunk 353/388: 5000 registros\n",
      "2026-01-05 17:21:04,192 - INFO - ğŸ“ˆ Chunk 354/388: 5000 registros\n",
      "2026-01-05 17:21:04,796 - INFO - ğŸ“ˆ Chunk 355/388: 5000 registros\n",
      "2026-01-05 17:21:05,376 - INFO - ğŸ“ˆ Chunk 356/388: 5000 registros\n",
      "2026-01-05 17:21:05,951 - INFO - ğŸ“ˆ Chunk 357/388: 5000 registros\n",
      "2026-01-05 17:21:06,528 - INFO - ğŸ“ˆ Chunk 358/388: 5000 registros\n",
      "2026-01-05 17:21:07,137 - INFO - ğŸ“ˆ Chunk 359/388: 5000 registros\n",
      "2026-01-05 17:21:07,746 - INFO - ğŸ“ˆ Chunk 360/388: 5000 registros\n",
      "2026-01-05 17:21:08,319 - INFO - ğŸ“ˆ Chunk 361/388: 5000 registros\n",
      "2026-01-05 17:21:08,890 - INFO - ğŸ“ˆ Chunk 362/388: 5000 registros\n",
      "2026-01-05 17:21:09,499 - INFO - ğŸ“ˆ Chunk 363/388: 5000 registros\n",
      "2026-01-05 17:21:10,139 - INFO - ğŸ“ˆ Chunk 364/388: 5000 registros\n",
      "2026-01-05 17:21:10,705 - INFO - ğŸ“ˆ Chunk 365/388: 5000 registros\n",
      "2026-01-05 17:21:11,289 - INFO - ğŸ“ˆ Chunk 366/388: 5000 registros\n",
      "2026-01-05 17:21:11,864 - INFO - ğŸ“ˆ Chunk 367/388: 5000 registros\n",
      "2026-01-05 17:21:12,543 - INFO - ğŸ“ˆ Chunk 368/388: 5000 registros\n",
      "2026-01-05 17:21:13,127 - INFO - ğŸ“ˆ Chunk 369/388: 5000 registros\n",
      "2026-01-05 17:21:13,712 - INFO - ğŸ“ˆ Chunk 370/388: 5000 registros\n",
      "2026-01-05 17:21:14,296 - INFO - ğŸ“ˆ Chunk 371/388: 5000 registros\n",
      "2026-01-05 17:21:14,868 - INFO - ğŸ“ˆ Chunk 372/388: 5000 registros\n",
      "2026-01-05 17:21:15,481 - INFO - ğŸ“ˆ Chunk 373/388: 5000 registros\n",
      "2026-01-05 17:21:16,098 - INFO - ğŸ“ˆ Chunk 374/388: 5000 registros\n",
      "2026-01-05 17:21:16,711 - INFO - ğŸ“ˆ Chunk 375/388: 5000 registros\n",
      "2026-01-05 17:21:17,312 - INFO - ğŸ“ˆ Chunk 376/388: 5000 registros\n",
      "2026-01-05 17:21:17,945 - INFO - ğŸ“ˆ Chunk 377/388: 5000 registros\n",
      "2026-01-05 17:21:18,560 - INFO - ğŸ“ˆ Chunk 378/388: 5000 registros\n",
      "2026-01-05 17:21:19,170 - INFO - ğŸ“ˆ Chunk 379/388: 5000 registros\n",
      "2026-01-05 17:21:19,751 - INFO - ğŸ“ˆ Chunk 380/388: 5000 registros\n",
      "2026-01-05 17:21:20,335 - INFO - ğŸ“ˆ Chunk 381/388: 5000 registros\n",
      "2026-01-05 17:21:20,966 - INFO - ğŸ“ˆ Chunk 382/388: 5000 registros\n",
      "2026-01-05 17:21:21,553 - INFO - ğŸ“ˆ Chunk 383/388: 5000 registros\n",
      "2026-01-05 17:21:22,166 - INFO - ğŸ“ˆ Chunk 384/388: 5000 registros\n",
      "2026-01-05 17:21:22,764 - INFO - ğŸ“ˆ Chunk 385/388: 5000 registros\n",
      "2026-01-05 17:21:23,408 - INFO - ğŸ“ˆ Chunk 386/388: 5000 registros\n",
      "2026-01-05 17:21:23,991 - INFO - ğŸ“ˆ Chunk 387/388: 5000 registros\n",
      "2026-01-05 17:21:24,074 - INFO - ğŸ“ˆ Chunk 388/388: 733 registros\n",
      "2026-01-05 17:21:24,075 - INFO - âœ… 1935733 ventas cargadas\n",
      "2026-01-05 17:21:24,095 - INFO - ğŸ”‘ Creando Ã­ndices de optimizaciÃ³n...\n",
      "2026-01-05 17:21:24,559 - INFO - âœ… Ãndices creados exitosamente\n",
      "2026-01-05 17:21:24,559 - INFO - ğŸ” Verificando carga de datos...\n",
      "2026-01-05 17:21:24,702 - INFO - ğŸ“Š VERIFICACIÃ“N DE CARGA:\n",
      "2026-01-05 17:21:24,703 - INFO -   ğŸ“ˆ Total ventas: 1,935,733\n",
      "2026-01-05 17:21:24,703 - INFO -   ğŸ“¦ Total productos: 12\n",
      "2026-01-05 17:21:24,703 - INFO -   ğŸŒ Total regiones: 21\n",
      "2026-01-05 17:21:24,703 - INFO -   ğŸ“… Rango fechas: 2025-12-05 hasta 2026-01-03\n",
      "2026-01-05 17:21:24,704 - INFO -   ğŸŒ Top 5 paÃ­ses:\n",
      "2026-01-05 17:21:24,704 - INFO -     COLOMBIA: 646,354 ventas\n",
      "2026-01-05 17:21:24,704 - INFO -     BRASIL: 644,875 ventas\n",
      "2026-01-05 17:21:24,705 - INFO -     MÃ‰XICO: 644,504 ventas\n",
      "2026-01-05 17:21:24,705 - ERROR - âŒ Error guardando estadÃ­sticas: name 'json' is not defined\n",
      "2026-01-05 17:21:24,706 - INFO - ============================================================\n",
      "2026-01-05 17:21:24,706 - INFO - âœ… CARGA FINAL COMPLETADA EXITOSAMENTE\n",
      "2026-01-05 17:21:24,706 - INFO - ============================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ Â¡Carga a PostgreSQL completada!\n",
      "ğŸ“Š Revisa el log: riwi_loader.log\n",
      "ğŸ“ˆ Revisa las estadÃ­sticas: riwi_carga_estadisticas.json\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # CONFIGURACIÃ“N - CAMBIA ESTOS VALORES\n",
    "    ARCHIVO_CSV_LIMPIO = \"riwi_datos_limpios.csv\"  # âš ï¸ CAMBIA SI ES NECESARIO\n",
    "    \n",
    "    # Crear cargador\n",
    "    cargador = PostgreSQLLoaderFinal(DB_CONFIG, ARCHIVO_CSV_LIMPIO)\n",
    "    \n",
    "    # Ejecutar carga\n",
    "    exito = cargador.ejecutar_carga_completa()\n",
    "    \n",
    "    if exito:\n",
    "        print(\"ğŸ‰ Â¡Carga a PostgreSQL completada!\")\n",
    "        print(\"ğŸ“Š Revisa el log: riwi_loader.log\")\n",
    "        print(\"ğŸ“ˆ Revisa las estadÃ­sticas: riwi_carga_estadisticas.json\")\n",
    "    else:\n",
    "        print(\"âŒ Carga fallida. Revisa el log.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
